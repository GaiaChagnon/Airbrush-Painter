# HPO search space v1
# Used by: scripts/train.py (with --hpo flag)
# Purpose: Optuna study configuration for hyperparameter optimization

schema: hpo_search_space.v1

# Objective function
objective:
  name: "avg_final_lpips_validation"
  direction: "minimize"          # Lower LPIPS is better (minimize directly)

# Trial budget
budget:
  n_trials: 80
  timeout_minutes: 3600          # 60 hours max

# Sampler and pruner
sampler:
  type: "tpe"                    # Tree-structured Parzen Estimator
  seed: 123

pruner:
  type: "median"
  warmup_steps: 8                # Don't prune before 8 epochs

# Search space (hyperparameters)
# NOTE: Paths updated to match rl_games structure in train.yaml
search_space:
  # RL-Games PPO hyperparameters (params.config.*)
  params.config.learning_rate:
    dist: "loguniform"
    low: 3.0e-5
    high: 3.0e-3
  params.config.entropy_coef:
    dist: "loguniform"
    low: 1.0e-4
    high: 1.0e-2
  params.config.gamma:
    dist: "uniform"
    low: 0.94
    high: 0.999
  params.config.e_clip:              # Renamed from agent.clip_param
    dist: "uniform"
    low: 0.1
    high: 0.3
  params.config.tau:                 # GAE lambda (new parameter)
    dist: "uniform"
    low: 0.90
    high: 0.99
  params.config.horizon_length:      # Rollout length (new parameter)
    dist: "int_uniform"
    low: 128
    high: 512
    step: 64
  params.config.mini_epochs:         # SGD passes (new parameter)
    dist: "int_uniform"
    low: 2
    high: 8
    step: 1
  
  # Custom agent architecture (agent.*)
  agent.softargmax_temp:
    dist: "loguniform"
    low: 0.5
    high: 3.0

# Validation set (fixed 10 images)
validation:
  image_dir: "data/validation_images/cmy_only/"
  stroke_cap: 1500               # Full cap for production-representative eval

# Constraints
constraints:
  max_strokes: 1500

# MLflow tracking
mlflow:
  experiment: "airbrush_hpo_v2"
  run_name_prefix: "trial"

